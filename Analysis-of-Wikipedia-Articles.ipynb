{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "path=r\"C:\\Users\\zain\\pf\\wiki\"\n",
    "paths=os.path.join(os.getcwd(),'wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles=[]\n",
    "for root, folders, files in os.walk(paths):\n",
    "    for file in files:\n",
    "        myf=os.path.join(root,file)\n",
    "        with open(myf,encoding=\"utf8\")as inf:\n",
    "            allfiles.append(inf.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols=['!','@','#','$','%','^','&','*','(',')','-','+','_','=','[',\n",
    "        ']','{','}',',',';',':','>','<','?','/','.',\"\\n\"]\n",
    "for i in range (0,len(allfiles)):\n",
    "    for j in symbols:\n",
    "        s=allfiles[i].replace(j,\"\")\n",
    "        allfiles[i]=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords by importing nltk library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.append(\"ndash\")\n",
    "stop_words.append(\"also\")\n",
    "for l in range(0, len(allfiles)):\n",
    "    m=allfiles[l].split()\n",
    "    for i in range(0, len(m)):\n",
    "        m[i]=m[i].strip()\n",
    "        m[i]=m[i].lower()\n",
    "        for j in stop_words:\n",
    "            if m[i]==j:\n",
    "                m[i]=\"\"\n",
    "    mystr=' '.join(map(str,m))\n",
    "    allfiles[l]=mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "l=WordNetLemmatizer()\n",
    "newart=[]\n",
    "for i in allfiles:\n",
    "    newart.append(l.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing K means Clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "v=TfidfVectorizer(stop_words='english',strip_accents = 'ascii',token_pattern=r'(?u)\\b[A-Za-z]{5,}\\b')\n",
    "#TFIDF vectorizer can also be used to remove stopwords, non english characters and any words that contain less/more values\n",
    "y=v.fit_transform(newart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=8\n",
    "mdl=KMeans(n_clusters=cluster, init='k-means++', max_iter=100, n_init=1)\n",
    "mdl.fit(y)\n",
    "print(\"Clusters\")\n",
    "cent=mdl.cluster_centers_.argsort()[:,::-1]\n",
    "t= v.get_feature_names()\n",
    "for i in range(cluster):\n",
    "    print(\"Cluster :\",i )\n",
    "    for i in cent[i, :10]:\n",
    "        print(' %s'%t[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
